# Environment Configuration for Voice Gateway with OpenWakeWord

# Node Environment
NODE_ENV=development

# Logging
LOG_LEVEL=info

# OpenWakeWord Configuration
# No API key required! OpenWakeWord is completely free and open-source

# Wake word model selection - use SHORT ALIAS or full path:
# SHORT ALIASES (recommended):
#   - jarvis  → models/hey_jarvis_v0.1.onnx (16 embedding frames)
#   - robot   → models/hello_robot.onnx (28 embedding frames)
# FULL PATHS (for custom models):
#   - models/custom_model.onnx
#
# Using aliases is recommended - they automatically set the correct embedding frames
OWW_MODEL_PATH=jarvis

# Threshold: 0.5 is standard, but you may need to lower to 0.01-0.05 for initial testing
# macOS users: Try 0.01 first, adjust based on scores you see in logs
# Raspberry Pi: 0.5 should work well
OWW_THRESHOLD=0.01

# Inference framework (keep as 'onnx')
OWW_INFERENCE_FRAMEWORK=onnx

# Embedding frames (auto-detected based on model, manual override available)
# hey_jarvis_v0.1.onnx: 16 frames
# hello_robot.onnx: 28 frames
# Leave commented to auto-detect based on OWW_MODEL_PATH
# OWW_EMBEDDING_FRAMES=16

# Detector warm-up duration in milliseconds (default: 1500)
# Time to wait after embedding buffer fills before accepting wake words
# Reduced from 2500ms to 1500ms for faster boot time
# Increase if experiencing false wake word detections during startup
# DETECTOR_WARMUP_MS=1500

# Audio Configuration
AUDIO_MIC_DEVICE=hw:2,0
AUDIO_SPEAKER_DEVICE=hw:2,0
AUDIO_SAMPLE_RATE=16000
AUDIO_CHANNELS=1
# Beep volume (0.0-1.0, default: 0.3 for 30% volume)
BEEP_VOLUME=0.3

# Voice Activity Detection (VAD)
# Silence threshold (RMS energy) - default: 0.003
# Lower values (e.g., 0.001-0.002): More sensitive, captures quieter speech, may pick up background noise
# Higher values (e.g., 0.005-0.01): Less sensitive, may miss quiet speech
# Adjust based on your microphone sensitivity and speaking volume
# VAD_SILENCE_THRESHOLD=0.003
VAD_TRAILING_SILENCE_MS=1500
VAD_MAX_UTTERANCE_MS=10000

# Whisper Speech-to-Text
# tiny model: Fast (1.5s), good accuracy for clear speech (recommended)
# base model: Slower (6s), better accuracy in noisy environments
WHISPER_MODEL=tiny
WHISPER_MODEL_PATH=models/ggml-tiny.bin

# Whisper Performance Optimizations
# Number of CPU threads (default: 4, increase for faster transcription)
WHISPER_THREADS=4
# Language code - skips language detection (default: en)
# Use 'auto' for automatic detection (slower)
WHISPER_LANGUAGE=en
# Beam size for decoding (default: 1 for greedy/fast, 5 for more accurate)
WHISPER_BEAM_SIZE=1
# Best-of candidates (default: 1 for greedy/fast, 5 for more accurate)
WHISPER_BEST_OF=1

# MQTT Broker
MQTT_BROKER_URL=mqtt://localhost:1883
# Client ID base name (random suffix automatically appended to support multiple instances)
# Example: voice-gateway-oww → voice-gateway-oww-a3f2b9
MQTT_CLIENT_ID=voice-gateway-oww
MQTT_USERNAME=
MQTT_PASSWORD=

# Health Check
HEALTHCHECK_PORT=3002

# MCP Server Retry Configuration
# Number of connection attempts before giving up (default: 2)
# Most MCP failures are immediate and permanent (server not installed, wrong path, etc.)
# Reduced from 3 attempts to minimize boot delay on failure
MCP_RETRY_ATTEMPTS=2
# Base delay in milliseconds for exponential backoff (default: 1000)
# Retry delays: attempt 1 (0ms), attempt 2 (1000ms)
# Reduced from 2000ms to speed up boot time without sacrificing reliability
MCP_RETRY_BASE_DELAY=1000

# ============================================================================
# AI PROVIDER CONFIGURATION
# ============================================================================
# Set AI_PROVIDER to 'anthropic' or 'ollama'
# Default: anthropic (use --ollama flag to switch to Ollama)
AI_PROVIDER=anthropic

# Custom system prompt (optional)
# If not set, uses default: "You are a helpful home automation assistant..."
# Example: AI_SYSTEM_PROMPT="You are Jarvis. Your favorite food is pizza. Answer in 1 sentence or less."
# AI_SYSTEM_PROMPT=

# ----------------------------------------------------------------------------
# Anthropic AI Configuration
# ----------------------------------------------------------------------------
# Get your API key from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=your_anthropic_api_key_here
# Valid models (as of Jan 2025):
#   - claude-3-5-haiku-20241022   (fastest, cheapest)
#   - claude-sonnet-4-5-20250929  (balanced, recommended)
#   - claude-opus-4-5-20251101    (most capable, slowest)
ANTHROPIC_MODEL=claude-3-5-haiku-20241022

# ----------------------------------------------------------------------------
# Ollama AI Configuration (Local AI)
# ----------------------------------------------------------------------------
OLLAMA_BASE_URL=http://localhost:11434
# qwen3:0.6b - Recommended (fast, good accuracy, supports tools)
# Note: qwen2.5 models do NOT handle tools properly and won't work with this app
OLLAMA_MODEL=qwen3:0.6b
# Disable thinking mode for qwen3 models (default: false)
# When true: Much faster responses (40s -> 2-5s) but may be less accurate
# When false: Uses full reasoning (slower but more accurate for complex queries)
OLLAMA_NO_THINK=false

# ============================================================================
# TEXT-TO-SPEECH CONFIGURATION
# ============================================================================
# TTS Provider: 'ElevenLabs' (cloud, high quality) or 'Piper' (local, offline)
TTS_PROVIDER=ElevenLabs

# Common TTS Settings (applies to all providers)
TTS_ENABLED=true
# Volume (0.0 to 1.0)
TTS_VOLUME=1.0
# Speed multiplier (0.5 = slower, 2.0 = faster)
# Note: ElevenLabs doesn't support speed adjustment via API
TTS_SPEED=1.0

# ----------------------------------------------------------------------------
# ElevenLabs TTS Configuration (Cloud TTS)
# ----------------------------------------------------------------------------
# Get your API key from: https://elevenlabs.io/app/settings/api-keys
ELEVENLABS_API_KEY=your_api_key_here
# Voice ID - Default: JBFqnCBsd6RMkjVDRZzb (George - deep, authoritative male)
# Browse voices at: https://elevenlabs.io/app/voice-library
# Popular voices:
#   - JBFqnCBsd6RMkjVDRZzb: George (deep male)
#   - EXAVITQu4vr4xnSDxMaL: Bella (soft female)
#   - pNInz6obpgDQGcFmaJgB: Adam (male narrator)
ELEVENLABS_VOICE_ID=JBFqnCBsd6RMkjVDRZzb
# Model ID - Options: eleven_multilingual_v2, eleven_turbo_v2_5, eleven_flash_v2_5
ELEVENLABS_MODEL_ID=eleven_multilingual_v2
# Voice settings (0.0 to 1.0)
ELEVENLABS_STABILITY=0.5
ELEVENLABS_SIMILARITY_BOOST=0.75
ELEVENLABS_STYLE=0.0
ELEVENLABS_USE_SPEAKER_BOOST=true

# ----------------------------------------------------------------------------
# Piper TTS Configuration (Local/Offline TTS)
# ----------------------------------------------------------------------------
# Path to Piper voice model file (.onnx format)
# Download voices from: https://github.com/rhasspy/piper/releases
# Example models:
#   - en_US-lessac-medium.onnx (female, balanced quality)
#   - en_US-amy-medium.onnx (female, natural)
#   - en_GB-alan-medium.onnx (male, British)
TTS_MODEL_PATH=models/piper/en_US-lessac-medium.onnx
# Dependencies: Python 3 + piper-tts package
# Install: pip3 install piper-tts
# Or see: https://github.com/rhasspy/piper for installation instructions

# Z-Wave MCP Server Configuration
# URL to zwave-js-ui instance (HTTP API)
# Z-Wave JS UI runs on port 8091 by default
# Example: http://10.0.0.58:8091 or http://localhost:8091
ZWAVE_UI_URL=http://localhost:8091
# Authentication (optional, set ZWAVE_UI_AUTH_ENABLED=true if needed)
ZWAVE_UI_AUTH_ENABLED=false
ZWAVE_UI_USERNAME=
ZWAVE_UI_PASSWORD=
# Socket timeout in milliseconds (optional)
ZWAVE_UI_SOCKET_TIMEOUT_MS=30000
