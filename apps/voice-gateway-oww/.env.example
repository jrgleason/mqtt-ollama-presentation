# Environment Configuration for Voice Gateway with OpenWakeWord

# Node Environment
NODE_ENV=development

# Logging
LOG_LEVEL=info

# OpenWakeWord Configuration
# No API key required! OpenWakeWord is completely free and open-source

# Wake word model selection:
# - hey_jarvis_v0.1.onnx (requires 16 embedding frames)
# - hello_robot.onnx (requires 28 embedding frames)
OWW_MODEL_PATH=models/hey_jarvis_v0.1.onnx

# Threshold: 0.5 is standard, but you may need to lower to 0.01-0.05 for initial testing
# macOS users: Try 0.01 first, adjust based on scores you see in logs
# Raspberry Pi: 0.5 should work well
OWW_THRESHOLD=0.01

# Inference framework (keep as 'onnx')
OWW_INFERENCE_FRAMEWORK=onnx

# Embedding frames (auto-detected based on model, manual override available)
# hey_jarvis_v0.1.onnx: 16 frames
# hello_robot.onnx: 28 frames
# Leave commented to auto-detect based on OWW_MODEL_PATH
# OWW_EMBEDDING_FRAMES=16

# Audio Configuration
AUDIO_MIC_DEVICE=hw:2,0
AUDIO_SPEAKER_DEVICE=hw:2,0
AUDIO_SAMPLE_RATE=16000
AUDIO_CHANNELS=1
# Beep volume (0.0-1.0, default: 0.3 for 30% volume)
BEEP_VOLUME=0.3

# Voice Activity Detection (VAD)
VAD_TRAILING_SILENCE_MS=1500
VAD_MAX_UTTERANCE_MS=10000

# Whisper Speech-to-Text
# tiny model: Fast (1.5s), good accuracy for clear speech (recommended)
# base model: Slower (6s), better accuracy in noisy environments
WHISPER_MODEL=tiny
WHISPER_MODEL_PATH=models/ggml-tiny.bin

# MQTT Broker
MQTT_BROKER_URL=mqtt://localhost:1883
MQTT_CLIENT_ID=voice-gateway-oww
MQTT_USERNAME=
MQTT_PASSWORD=

# Health Check
HEALTHCHECK_PORT=3002

# AI Provider Configuration
# Set AI_PROVIDER to 'anthropic' or 'ollama'
# Default: anthropic (use --ollama flag to switch to Ollama)
AI_PROVIDER=anthropic

# Anthropic AI Configuration
# Get your API key from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=your_anthropic_api_key_here
# Model: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022, claude-3-opus-20240229
ANTHROPIC_MODEL=claude-3-5-haiku-20241022

# Ollama AI Configuration
OLLAMA_BASE_URL=http://localhost:11434
# qwen2.5:0.5b - Fastest (1s), good for simple queries (recommended for voice)
# qwen2.5:1.5b - Better accuracy (4.6s), use if quality matters more than speed
# qwen3:1.7b - Legacy model, not recommended (14s, superseded by qwen2.5)
OLLAMA_MODEL=qwen2.5:0.5b

# Text-to-Speech (ElevenLabs)
# Set to false to disable TTS output
TTS_ENABLED=true
# Volume (0.0 to 1.0)
TTS_VOLUME=1.0
# Speed multiplier (0.5 = slower, 2.0 = faster) - Note: ElevenLabs doesn't support speed adjustment via API
TTS_SPEED=1.0

# ElevenLabs API Configuration
# Get your API key from: https://elevenlabs.io/app/settings/api-keys
ELEVENLABS_API_KEY=your_api_key_here
# Voice ID - Default: JBFqnCBsd6RMkjVDRZzb (George - deep, authoritative male)
# Browse voices at: https://elevenlabs.io/app/voice-library
# Popular voices:
#   - JBFqnCBsd6RMkjVDRZzb: George (deep male)
#   - EXAVITQu4vr4xnSDxMaL: Bella (soft female)
#   - pNInz6obpgDQGcFmaJgB: Adam (male narrator)
ELEVENLABS_VOICE_ID=JBFqnCBsd6RMkjVDRZzb
# Model ID - Options: eleven_multilingual_v2, eleven_turbo_v2_5, eleven_flash_v2_5
ELEVENLABS_MODEL_ID=eleven_multilingual_v2
# Voice settings (0.0 to 1.0)
ELEVENLABS_STABILITY=0.5
ELEVENLABS_SIMILARITY_BOOST=0.75
ELEVENLABS_STYLE=0.0
ELEVENLABS_USE_SPEAKER_BOOST=true

# Z-Wave MCP Server Configuration
# URL to zwave-js-ui instance (HTTP API)
# Z-Wave JS UI runs on port 8091 by default
# Example: http://10.0.0.58:8091 or http://localhost:8091
ZWAVE_UI_URL=http://localhost:8091
# Authentication (optional, set ZWAVE_UI_AUTH_ENABLED=true if needed)
ZWAVE_UI_AUTH_ENABLED=false
ZWAVE_UI_USERNAME=
ZWAVE_UI_PASSWORD=
# Socket timeout in milliseconds (optional)
ZWAVE_UI_SOCKET_TIMEOUT_MS=30000
